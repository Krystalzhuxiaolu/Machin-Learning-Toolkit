{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5abf3f60dc254cd656abb480d766db2d60a64423ce92680f144b473b08e2803f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "# Import your dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Data.csv\")\n",
    "X = dataset.iloc[:,1:-1].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "source": [
    "# Split your data into training and testing sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "source": [
    "# Feature scaling (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.fit_transform(X_test)\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "y_test = sc_y.fit_transform(y_test)"
   ]
  },
  {
   "source": [
    "# Select your classifier\n",
    "## Logistic Regression: from sklearn.linear_model import LogisticRegression classifier = LogisticRegression(random_state = 0)\n",
    "## KNN: from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "## SVM(Linear Kernel): from sklearn.svm import SVC  classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "## SVM(Non-linear Kernel): from sklearn.svm import SVC classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "## Naive Bayes: from sklearn.naive_bayes import GaussianNB classifier = GaussianNB()\n",
    "## Decision Tree: from sklearn.tree import DecisionTreeClassifier classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "## Random Forest: from sklearn.ensemble import RandomForestClassifier classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "from sklearn.svm import SVC \n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "from sklearn.svm import \n",
    "SVC classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "from sklearn.naive_bayes \n",
    "import GaussianNB classifier = GaussianNB()\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models =  [(\"LR\", LogisticRegression()), \n",
    "            (\"LDA\", LinearDiscriminantAnalysis()), \n",
    "            (\"QDA\", QuadraticDiscriminantAnalysis()), \n",
    "            (\"LSVC\", LinearSVC()), \n",
    "            (\"RSVM\", SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)),              \n",
    "            (\"RF\", RandomForestClassifier( n_estimators=1000, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='auto', bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0))]\n",
    "\n",
    "for m in models:\n",
    "    #train each of the models on the training set\n",
    "    m[1].fit(X_train,y_train)\n",
    "\n",
    "    #make an array of predictions on the test set\n",
    "    pred = m[1].predict(X_test)\n",
    "\n",
    "    #output the hit-rate and the confusion matrix for each model\n",
    "    print(\"%s:\\n%0.3f\" % (m[0],m[1].score(X_test,y_test)))\n",
    "    print(\"%s:\\n\" % confusion_matrix(pred,y_test))\n",
    "    print(type(pred))"
   ]
  },
  {
   "source": [
    "# Evaluating your performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "# Visualising the Training set results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Visualising the Test set results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "- Logistic Regression or Naive Bayes when you want to rank your predictions by their probability. For example if you want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually that allows you to target your marketing campaigns. And of course for this type of business problem, you should use Logistic Regression if your problem is linear, and Naive Bayes if your problem is non linear.\n",
    "\n",
    "- SVM when you want to predict to which segment your customers belong to. Segments can be any kind of segments, for example some market segments you identified earlier with clustering.\n",
    "\n",
    "- Decision Tree when you want to have clear interpretation of your model results,\n",
    "\n",
    "- Random Forest when you are just looking for high performance with less need for interpretation. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}